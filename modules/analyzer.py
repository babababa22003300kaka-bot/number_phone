#!/usr/bin/env python3
"""Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙØ­Ø³Ù‘Ù† (AsyncIO) - Ø§Ù„Ù†Ø³Ø®Ø© 2.4 (Phase 3)"""

import re
import gc
import httpx
from bs4 import BeautifulSoup
from html import unescape
from typing import Dict, List, Optional, Tuple
from urllib.parse import urljoin, urlparse

class WebAnalyzer:
    """Ù…Ø­Ù„Ù„ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ø°ÙƒÙŠ (Ù†Ø³Ø®Ø© AsyncIO + Browser Fallback)"""
    
    def __init__(
        self, 
        html_keywords: List[str],
        api_keywords: List[str],
        exclude_keywords: List[str] = None,
        timeout: int = 10,
        max_size: int = 3145728,
        user_agent: str = None,
        browser_service_url: str = None,
        fallback_threshold: int = 20,
        proxy_config: Optional[Dict] = None
    ):
        self.html_keywords = [k.lower() for k in html_keywords]
        self.api_keywords = [k.lower() for k in api_keywords]
        self.exclude_keywords = [k.lower() for k in (exclude_keywords or [])]
        self.timeout = timeout
        self.max_size = max_size
        self.browser_service_url = browser_service_url
        self.fallback_threshold = fallback_threshold
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠØ§Øª (if proxy support configured)
        if proxy_config:
            from modules.proxy_manager import get_proxy_list, choose_proxy, mask_proxy_url
            self.proxy_list = get_proxy_list(proxy_config)
            self.proxy_config = proxy_config
            proxy_url = choose_proxy(
                self.proxy_list,
                rotate=proxy_config.get('proxy', {}).get('rotate', True)
            )
            self.proxy_url = proxy_url
            if proxy_url:
                print(f"ğŸŒ Proxy enabled: {mask_proxy_url(proxy_url)}")
        else:
            self.proxy_url = None
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù€ AsyncClient
        self.client = httpx.AsyncClient(
            timeout=timeout,
            follow_redirects=True,
            limits=httpx.Limits(max_connections=200, max_keepalive_connections=50),
            headers={
                "User-Agent": user_agent or "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "en-US,en;q=0.9,ar;q=0.8"
            },
            verify=False  # ØªØ¬Ø§Ù‡Ù„ Ø£Ø®Ø·Ø§Ø¡ SSL Ù„Ù„Ø³Ø±Ø¹Ø©
        )
    
    def _check_exclusion(self, html: str) -> Tuple[bool, str]:
        """ÙØ­Øµ ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ ÙÙŠ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙØ­Ø©"""
        html_lower = html.lower()
        for keyword in self.exclude_keywords:
            if keyword in html_lower:
                return True, keyword
        return False, ""
    
    def detect_protection(self, response: httpx.Response, html: str) -> Tuple[bool, str]:
        """ÙƒØ´Ù Cloudflare/Captcha"""
        # Cloudflare
        if 'cf-ray' in response.headers or 'cloudflare' in html.lower():
            return True, "cloudflare"
        
        # Captcha
        captcha_indicators = ['recaptcha', 'hcaptcha', 'captcha', 'g-recaptcha']
        if any(ind in html.lower() for ind in captcha_indicators):
            return True, "captcha"
        
        return False, "clean"
    
    def analyze_inputs(self, soup: BeautifulSoup) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ø­Ù‚ÙˆÙ„ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„"""
        phone_score = 0
        verify_score = 0
        evidence = []
        
        for inp in soup.find_all(["input", "textarea", "select"]):
            attrs_text = " ".join([
                str(inp.get(a, "")) 
                for a in ["id", "name", "class", "placeholder", "type", 
                         "inputmode", "autocomplete", "aria-label"]
            ]).lower()
            
            if inp.get("type") == "tel" or inp.get("inputmode") == "tel":
                phone_score += 30
                evidence.append({
                    "type": "input_type_tel",
                    "confidence": "high",
                    "element": str(inp)[:200]
                })
            
            if "tel" in inp.get("autocomplete", "").lower():
                phone_score += 20
            
            for kw in self.html_keywords:
                if kw in attrs_text:
                    if kw in ["phone", "mobile", "tel"]:
                        phone_score += 15
                    elif kw in ["verify", "otp", "code"]:
                        verify_score += 15
                    evidence.append({
                        "type": "input_keyword",
                        "keyword": kw,
                        "element": str(inp)[:200]
                    })
            
            pattern = inp.get("pattern", "")
            if pattern and re.search(r'[\d\+\-\(\)]{6,}', pattern):
                phone_score += 10
        
        return {
            "phone_score": min(phone_score, 100),
            "verify_score": min(verify_score, 100),
            "evidence": evidence[:10]
        }
    
    def analyze_text(self, soup: BeautifulSoup) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„Ù€ labels"""
        phone_score = 0
        verify_score = 0
        evidence = []
        
        for tag in soup.find_all(["label", "button", "a", "h1", "h2", "h3", "span"]):
            text = unescape(" ".join(tag.stripped_strings)).lower()
            if not text or len(text) > 200:
                continue
            
            for kw in self.html_keywords:
                if kw in text:
                    if kw in ["phone", "mobile", "signup", "register"]:
                        phone_score += 5
                    elif kw in ["verify", "otp", "send"]:
                        verify_score += 10
                    evidence.append({
                        "type": "text_keyword",
                        "keyword": kw,
                        "snippet": text[:100]
                    })
        
        return {
            "phone_score": min(phone_score, 50),
            "verify_score": min(verify_score, 50),
            "evidence": evidence[:10]
        }
    
    def analyze_api(self, soup: BeautifulSoup, html: str) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù€ API endpoints"""
        verify_score = 0
        endpoints = []
        
        scripts = soup.find_all("script")
        script_text = " ".join([s.get_text() for s in scripts if s.string])
        
        url_pattern = r'(?:https?://[^\s"\'\)<>]+|/api/[^\s"\'\)<>]+|/v\d+/[^\s"\'\)<>]+)'
        found_urls = re.findall(url_pattern, html + script_text)
        
        for url in found_urls:
            url_lower = url.lower()
            for kw in self.api_keywords:
                if kw in url_lower:
                    verify_score += 20
                    endpoints.append({
                        "url": url,
                        "keyword": kw,
                        "confidence": "high"
                    })
                    break
        
        api_patterns = [
            r'(send(?:Otp|OTP|Sms|SMS|Code|Verification))',
            r'(verify(?:Otp|OTP|Code|Phone|SMS))',
            r'(check(?:Otp|OTP|Phone))'
        ]
        
        for pattern in api_patterns:
            matches = re.findall(pattern, script_text, re.IGNORECASE)
            for match in matches[:5]:
                verify_score += 15
                endpoints.append({
                    "function": match,
                    "confidence": "medium"
                })
        
        return {
            "verify_score": min(verify_score, 100),
            "endpoints": endpoints[:10],
            "script_text": script_text
        }
    
    def _check_signatures(self, html: str, script_text: str) -> Dict:
        """ÙƒØ´Ù Ø¨ØµÙ…Ø§Øª Ù…Ø²ÙˆØ¯ÙŠ Ø®Ø¯Ù…Ø§Øª OTP/SMS"""
        signatures = {
            "firebase": [
                r'firebase\.initializeApp',
                r'firebase\.auth\(\)',
                r'signInWithPhoneNumber',
                r'recaptcha-container'
            ],
            "twilio": [
                r'Twilio\.Device',
                r'Twilio\.Chat',
                r'api\.twilio\.com'
            ],
            "msg91": [
                r'msg91\.com',
                r'msg91'
            ],
            "infobip": [
                r'infobip',
                r'api\.infobip'
            ],
            "nexmo": [
                r'nexmo',
                r'vonage',
                r'api\.nexmo'
            ],
            "aws_sns": [
                r'aws-sdk',
                r'AWS\.SNS',
                r'sns\.amazonaws'
            ],
            "plivo": [
                r'plivo',
                r'api\.plivo'
            ],
            "messagebird": [
                r'messagebird',
                r'rest\.messagebird'
            ]
        }
        
        found_signatures = []
        score = 0
        combined_text = html + " " + script_text
        
        for provider, patterns in signatures.items():
            for pattern in patterns:
                if re.search(pattern, combined_text, re.IGNORECASE):
                    found_signatures.append(provider)
                    score += 25
                    break
        
        return {
            "score": min(score, 50),
            "signatures": found_signatures
        }
    
    def calculate_confidence(self, results: Dict) -> int:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©"""
        phone_score = results["phone_score"]
        verify_score = results["verify_score"]
        
        if phone_score >= 40 and verify_score >= 40:
            return min(phone_score + verify_score, 100)
        elif phone_score >= 50:
            return phone_score + (verify_score // 2)
        elif verify_score >= 50:
            return verify_score + (phone_score // 2)
        else:
            return (phone_score + verify_score) // 2

    async def check_paths(self, base_url: str, paths: List[str]) -> List[Dict]:
        """ÙØ­Øµ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„ÙØ±Ø¹ÙŠØ© (Path Fuzzing)"""
        found_paths = []
        
        for path in paths:
            full_url = urljoin(base_url, path)
            try:
                print(f"ğŸ” [PATH] Checking {full_url} ...", end="\r")
                # Use proxy if available
                if self.proxy_url:
                    response = await self.client.get(full_url, proxy=self.proxy_url)
                else:
                    response = await self.client.get(full_url)
                
                if response.status_code == 200:
                    html = response.text
                    soup = BeautifulSoup(html, "lxml")
                    inputs = self.analyze_inputs(soup)
                    text = self.analyze_text(soup)
                    
                    sub_score = inputs["phone_score"] + text["phone_score"] + inputs["verify_score"]
                    
                    if sub_score > 20:
                        found_paths.append({
                            "url": full_url,
                            "score": sub_score,
                            "title": soup.title.string.strip() if soup.title else "No Title"
                        })
            except:
                continue
                
        return found_paths
    
    async def _fallback_browser_check(self, url: str) -> Optional[str]:
        """Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ÙØ­Øµ Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BaaS Server"""
        if not self.browser_service_url:
            return None
        
        try:
            # Use proxy if available
            if self.proxy_url:
                response = await self.client.post(
                    self.browser_service_url,
                    json={"url": url, "timeout": 30000},
                    timeout=35.0,
                    proxy=self.proxy_url
                )
            else:
                response = await self.client.post(
                    self.browser_service_url,
                    json={"url": url, "timeout": 30000},
                    timeout=35.0
                )
            
            if response.status_code == 200:
                data = response.json()
                return data.get("html")
        except Exception as e:
            print(f"âš ï¸ [BaaS] Failed to render {url}: {str(e)}")
        
        return None

    async def analyze(self, url: str, scan_paths: List[str] = None) -> Optional[Dict]:
        """Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ (Async) Ù…Ø¹ Browser Fallback"""
        try:
            # Ø§Ù„Ø·Ù„Ø¨ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ (HTTPX Fast) with proxy support
            if self.proxy_url:
                response = await self.client.get(url, proxy=self.proxy_url)
            else:
                response = await self.client.get(url)
            
            if len(response.content) > self.max_size:
                return {"url": url, "status": "oversize", "confidence": 0}
            
            html = response.text
            
            # ÙØ­Øµ Ø§Ù„Ø§Ø³ØªØ¨Ø¹Ø§Ø¯
            is_excluded, excluded_keyword = self._check_exclusion(html)
            if is_excluded:
                return {
                    "url": url,
                    "status": "excluded",
                    "reason": excluded_keyword,
                    "confidence": 0
                }
            
            # ÙƒØ´Ù Ø§Ù„Ø­Ù…Ø§ÙŠØ©
            is_protected, protection_type = self.detect_protection(response, html)
            if is_protected:
                return {
                    "url": url,
                    "status": "protected",
                    "protection": protection_type,
                    "confidence": 0
                }
            
            # Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
            soup = BeautifulSoup(html, "lxml")
            inputs = self.analyze_inputs(soup)
            text = self.analyze_text(soup)
            api = self.analyze_api(soup, html)
            signatures = self._check_signatures(html, api.get("script_text", ""))
            
            phone_score = inputs["phone_score"] + text["phone_score"]
            verify_score = inputs["verify_score"] + text["verify_score"] + api["verify_score"] + signatures["score"]
            
            results = {
                "phone_score": min(phone_score, 100),
                "verify_score": min(verify_score, 100)
            }
            
            confidence = self.calculate_confidence(results)
            method = "httpx"
            
            # ğŸ”„ Browser Fallback (Phase 3)
            if self.browser_service_url and confidence < self.fallback_threshold:
                print(f"ğŸ”„ [RETRY] {url} with Browser (Conf: {confidence}% < {self.fallback_threshold}%)...")
                full_html = await self._fallback_browser_check(url)
                
                if full_html:
                    # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ù„Ù€ HTML Ø§Ù„Ø¬Ø¯ÙŠØ¯
                    soup = BeautifulSoup(full_html, "lxml")
                    inputs = self.analyze_inputs(soup)
                    text = self.analyze_text(soup)
                    api = self.analyze_api(soup, full_html)
                    signatures = self._check_signatures(full_html, api.get("script_text", ""))
                    
                    phone_score = inputs["phone_score"] + text["phone_score"]
                    verify_score = inputs["verify_score"] + text["verify_score"] + api["verify_score"] + signatures["score"]
                    
                    results = {
                        "phone_score": min(phone_score, 100),
                        "verify_score": min(verify_score, 100)
                    }
                    
                    confidence = self.calculate_confidence(results)
                    method = "playwright"
                    print(f"âœ… [BROWSER] New confidence: {confidence}%")
            
            # Path Fuzzing
            valid_paths = []
            if scan_paths and (confidence > 10 or response.status_code == 200):
                valid_paths = await self.check_paths(url, scan_paths)
                
                if valid_paths:
                    confidence = max(confidence, 80)
                    results["phone_score"] = max(results["phone_score"], 80)
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
            result = {
                "url": url,
                "status": "analyzed",
                "http_status": response.status_code,
                "confidence": confidence,
                "phone_score": results["phone_score"],
                "verify_score": results["verify_score"],
                "method": method,
                "evidence": {
                    "inputs": inputs["evidence"][:5],
                    "text": text["evidence"][:5],
                    "api": api["endpoints"][:5],
                    "signatures": signatures["signatures"],
                    "paths": valid_paths
                }
            }
            
            # âœ… ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
            del soup, html, response, inputs, text, api, signatures
            if valid_paths:
                del valid_paths
            
            # Force garbage collection
            gc.collect()
            
            return result
        
        except httpx.TimeoutException:
            gc.collect()  # ØªÙ†Ø¸ÙŠÙ Ø­ØªÙ‰ ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£
            return {"url": url, "status": "timeout", "confidence": 0}
        except httpx.ConnectError:
            gc.collect()
            return {"url": url, "status": "connection_error", "confidence": 0}
        except Exception as e:
            gc.collect()
            return {"url": url, "status": "error", "error": str(e), "confidence": 0}
    
    async def close(self):
        """Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ù€ client"""
        await self.client.aclose()